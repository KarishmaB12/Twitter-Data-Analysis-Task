{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Twitter Data Analysis</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DATA COLLECTION](#DATA-COLLECTION)\n",
    "1. Collect a random sample of 10K tweets using the Twitter API and store them in a MongoDB instance.\n",
    "2. From these collected tweets, parse the 5 most frequently occurring [named-entities](Entity Recognition & Sentiment Analysis.ipynb) (can be a name, person, location, product etc).\n",
    "3. Now, collect the [latest news from various news source APIs](Entity Recognition & Sentiment Analysis.ipynb) featuring the named-entities you got from Step 2 (use at least one other API/library other than Twitter's to collect this data).\n",
    "\n",
    "\n",
    "[ANALYSIS](#ANALYSIS)\n",
    "0. Perform a [Sentiment Analysis](Entity Recognition & Sentiment Analysis.ipynb) on the data collected in Step 1 and 3, and compare the twitter and news sentiments for the common named-entities. Also, do a qualitative comparison of the predicted sentiment versus the original sentiment of the tweet and news articles.\n",
    "9. You should also perform [temporal](Temporal Analysis Final.ipynb), [spatial](Spatial Analysis.ipynb) and [content analysis](Content Analysis.ipynb) on the collected data.\n",
    "0. Report these results you found in the steps 5 & 6 using graphs. Brownie points for cool interactive visualisations.\n",
    "\n",
    "\n",
    "[APPLICATION](#APPLICATION)\n",
    "7. Set up a web application on Heroku or Digital Ocean Droplet with a user interface where we can input a named-entity and get the comparison between the news and twitter sentiments as an output.\n",
    "8. Put all your code, along with the MongoDB collection, in a GitHub repository and share the link with us. Also, maintain a README.md explaining your codebase and the approach you took."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Please note:**_ The analysis is carried out in different Jupyter Notebooks. Please link on the links in the above cell to access the sheets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**For better and easier access:**_\n",
    "1. [Entity Recognition & Sentiment Analysis](Entity Recognition & Sentiment Analysis.ipynb)\n",
    "2. [Content Analysis](Content Analysis.ipynb)\n",
    "3. [Spatial Analysis](Spatial Analysis.ipynb)\n",
    "4. [Temporal Analysis](Temporal Analysis Final.ipynb)\n",
    "5. The [web application](analysis-twitter.herokuapp.com) has been hosted on Heroku. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Collection of 10,000 random tweets and stored in a MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing necessary packages\n",
    "import tweepy\n",
    "import pymongo\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variables that contains the user credentials to access Twitter API \n",
    "consumer_key = 'XeRPtmT6mBf0Yfd3IjPGhhv5x'\n",
    "consumer_secret = 'fHzH7CAJ1qujrugS2dZ8ZTkFL102lTL45X8zL5wmfKg5CSXphs'\n",
    "access_token = '984662830770253824-1W2kGkI2hIKrKJpBMaFEvkbjuV4rdlG'\n",
    "access_token_secret = 'shgWn5P5bHhIwwKy6qABHEmZ3cvc8odAUHliSA8hBMNMt'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a connection to MongoDB database 'EngTweetsDb'\n",
    "MONGO_HOST = 'mongodb://localhost/EngTweetsDb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a class StreamListener inheriting from 'StreamListener' and overriding on_connect, on_error, __init__, on_data\n",
    "class StreamListener(tweepy.StreamListener):    \n",
    "    #Class provided by tweepy to access the Twitter Streaming API. \n",
    "\n",
    "    def on_connect(self):\n",
    "        print(\"You are now connected to the streaming API.\")\n",
    " \n",
    "    def on_error(self, status_code):\n",
    "        print('An Error has occured: ' + repr(status_code))\n",
    "        return False\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super().__init__()\n",
    "        self.num_tweets = 0\n",
    "   \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            client = pymongo.MongoClient(MONGO_HOST)\n",
    "            db = client.EngTweetsDb\n",
    "            datajson = json.loads(data)\n",
    "            created_at = datajson['created_at']\n",
    "            print(\"Tweet collected at \" + str(created_at))\n",
    "            self.num_tweets += 1\n",
    "            print('Total number of tweets in the db collection= %s'% db.en_tweets_col.count())\n",
    "            if self.num_tweets < 12:\n",
    "                db.en_tweets_col.insert(datajson)\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the listener. The 'wait_on_rate_limit=True' is needed to help with Twitter API rate limiting.\n",
    "listener = StreamListener(api=tweepy.API(wait_on_rate_limit=True)) \n",
    "\n",
    "#Creating a stream\n",
    "streamer = tweepy.Stream(auth=auth, listener=listener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sampling the streaming tweets on the basis of language\n",
    "print(\"Tracking...\")\n",
    "streamer.sample(languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking the records stored in 'en_tweets_col' collection of 'EngTweetsDb'\n",
    "col = pymongo.MongoClient(MONGO_HOST)[\"EngTweetsDb\"][\"en_tweets_col\"]\n",
    "col.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
