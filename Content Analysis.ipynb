{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Content Analysis</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the MongoDB database 'EngTweetsDb' where the tweets are stored\n",
    "import pymongo\n",
    "MONGO_HOST = 'mongodb://localhost/EngTweetsDb'\n",
    "conn = pymongo.MongoClient(MONGO_HOST)[\"EngTweetsDb\"][\"en_tweets_col\"]\n",
    "conn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting only the tweet text from the database\n",
    "all_text_id =(list(conn.find({},{\"text\":1})))\n",
    "text_only = []\n",
    "for i in range(conn.count()):\n",
    "    text_str = all_text_id[i][\"text\"]\n",
    "    text_only.append(text_str)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using 're'package to remove numbers and symbols which are not needed for creating a word cloud\n",
    "import re\n",
    "text_refine = re.findall('[A-Za-z]+',str(text_only))\n",
    "#The data is now free of numbers, symbols and is now converted to lowercase\n",
    "text_refine_lower = [i.lower() for i in text_refine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 'NLTK' package to remove non-english words like messaging text format\n",
    "from nltk.corpus import wordnet\n",
    "text_refine_lower_valid_eng_words = []\n",
    "temp_len = len(text_refine_lower)\n",
    "\n",
    "for i in range(temp_len):\n",
    "    print(text_refine_lower[i])\n",
    "    if not wordnet.synsets(text_refine_lower[i]):\n",
    "        print('Not an ENGLISH word')\n",
    "        temp = text_refine_lower[i]\n",
    "        text_refine_lower.remove(temp)\n",
    "        temp_len = temp_len - 1\n",
    "        i = i + 1\n",
    "\n",
    "print(len(text_refine_lower))\n",
    "for i in range(len(text_refine_lower)):\n",
    "        temp_new = text_refine_lower[i]\n",
    "        text_refine_lower_valid_eng_words.append(temp_new)\n",
    "\n",
    "print(text_refine_lower_valid_eng_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using 'NLTK' to remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "text_refine_lower_valid_eng_words_filtered = []\n",
    " \n",
    "for w in text_refine_lower_valid_eng_words:\n",
    "    if w not in stopWords:\n",
    "        text_refine_lower_valid_eng_words_filtered.append(w)\n",
    "print(text_refine_lower_valid_eng_words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using 'wordcloud' package to make a word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(font_path='C:/Users/Dell user/Desktop/Professional/PhD/TASK/Resources/Verdana.ttf',\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=3000,\n",
    "                          height=2500\n",
    "                         ).generate(str(text_refine_lower_valid_eng_words_filtered))\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using 'NLTK' to find bigrams in tweet text\n",
    "from nltk import bigrams\n",
    "bigrm = list(bigrams(str(text_refine_lower).split()))\n",
    "print(*map(' '.join, bigrm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 'NLTK' to find bigram pairs\n",
    "from nltk.collocations import *\n",
    "import nltk\n",
    "tweet_phrases = str(text_refine_lower_valid_eng_words_filtered)\n",
    "finder = BigramCollocationFinder.from_words(tweet_phrases.split(), window_size = 3)\n",
    "finder1 = BigramCollocationFinder.from_words(tweet_phrases.split(), window_size = 3)\n",
    "finder1.apply_freq_filter(2)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "for k,v in finder.ngram_fd.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iii)TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 'sklearn' to perform Term Frequency-Inverse Document Frequency  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = text_refine_lower_valid_eng_words_filtered\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "print(dict(zip(vectorizer.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (iv) Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting the 'hashtags' information from the stored tweets \n",
    "hashtags =(list(conn.find({},{\"entities.hashtags.text\":1})))\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a list of the hashtags used in the tweets\n",
    "for i in range(len(hashtags)):\n",
    "    hashtags_str = hashtags[i][\"entities\"][\"hashtags\"]\n",
    "    hashtags_only.append(hashtags_str)\n",
    "    \n",
    "\n",
    "print(hashtags_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (v) Followers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting the 'followers_count' information about the twitter user\n",
    "followers_count = list(col.find({},{'user.followers_count':1}))\n",
    "followers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
